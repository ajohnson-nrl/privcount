#!/usr/bin/env python

import datetime
import os
import sys

from multiprocessing import Pool, cpu_count
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter

from numpy import mean
from stem import Flag
from stem.descriptor import parse_file
from stem.descriptor import DocumentHandler
from stem.descriptor.remote import DescriptorDownloader
from stem.version import Version

def get_unknown_privcount_version_stub():
    return 'unknown (>= PrivCount 1.0.1)'

try:
    from privcount.protocol import get_privcount_version
except:
    get_privcount_version = get_unknown_privcount_version_stub

'''
This is a general purpose tool that will search through a consenus or all
consensuses in a directory and compute the mean position weights that a
group of relays by fingerprints had over all consensuses.

Clients typically have a consensus that is between 1 and 3 hours old. You
should choose a range of consensuses that reflect this delay. (For example, if
you collect between 03:00 and 23:00, use consensuses 01:00 through 20:00.)

HSDir v2s rotate every 24 hours at a time based on each service's onion
address. To avoid introducing bias, you MUST collect v2 HSDir statistics for
a multiple of  24 hours.

HSDir v2s see one more set of descriptors than the number of days in the
collection period, because the changeovers are distributed throughout the
day. Therefore, any figures obtained from an N-day period need to be scaled by
N/(N+1).

This script provides simplified weights, which don't account for:
* exit ports, exit policies, and exit and rend long-lived ports
* HSDir v2 allocation variance due to fingerprint ranges
* HSDir v2 bias due to descriptor upload times in the first and last hour
* HSDir v3 allocation variance due to fingerprint ranges
  (this requires relay ed25519 keys, which are not in the consensus)
* overlapping HSDir descriptor replicas
* one descriptor per client for services using stealth authentication
* precise client consensus download delay distributions and overlap periods

For more details, see:
https://research.torproject.org/techreports/extrapolating-hidserv-stats-2015-01-31.pdf
https://trac.torproject.org/projects/tor/ticket/23367

Example execution (the '-' means download the latest consensus from Tor):

./compute_fractional_position_weights - 068308AD070849A71B8C1DB06C2509E82C40B908 11796EF96A84A328124B64383E768AFF90BA583F 11EAB5C9137906EF7E6A32365C4B37613698E647 1A4488A367D89D0EFDA88116059FEBCACF0F508A 1A58D4BF5B4A0AF0378F650D11A6569685C35C1D 363F42695F2DD825DA5A4E6ABF3FBDFCFD1E9AE2 3887BA09C064062B711125009404B372A90BF190 493CD90F239AE608D9C046C543C59ABB9815B4DB 87AA996EFB625724F8932EF789C761E33A66A83A A2D885A9B6F4D310372EA44FC0753F45CE44D1F7 B215FA3E0C37E71FF4330EA0A7D5F75D9BD2F2BE B6718125C43ECA2E5011B3C681BB6638617A9686 BA24CD0E76682C5AAFC09CC05476B94653699A39 C127882E54F77884F8D2764CB9744B1F2C94654B C6B3546CC6BCCB649FEC82D348D464554BC6323D CE46351D06E89C0047C34AF2CED26C441B57FDF0 DE684E6C6B7773B8BE74B4D941E4178988E15E26 F6D42724B636FDF05B0EAEBE5E830C586DCFD114 0DA9BD201766EDB19F57F49F1A013A8A5432C008 12B80ABF019354A9D25EE8BE85EB3C0AD8F7DFC1 890E2EA65455FBF0FAAB4159FAC4412BDCB24295 C170AE5A886C5A09D6D1CF5CF284653632EEF25D D52CD431CEF28E01B11F545A84347EE45524BCA7 EE21F83AB6F76E3B3FFCBA5C2496F789CB84E7C6 A5945077E0D35729F8E2920A54BE12A0058B403E D53793315E290D250E9AFC431A4C9068A1E53C98

Tests:

No fingerprints:
./compute_fractional_position_weights -

All fingerprints
./compute_fractional_position_weights - -
'''

# 3 is the default number of intro points per service
# TODO: work out the weighted mean of the number of intro points per service,
#       including services with stealth client auth
INTRO_ESTABLISH_PER_SERVICE = 3.0

# 3 and 2 are the hard-coded (v2) or default consensus parameter (v3) values
HSDIR_SPREAD = 3
HSDIR_REPLICA = 2

# The entire hash ring of v2 HSDirs will see each onion address stored 6 times
# per service upload
HSDIR_V2_ONION_ADDRESS_STORE_PER_SERVICE = HSDIR_SPREAD * HSDIR_REPLICA

# The entire hash ring of v2 HSDirs will see each onion address fetched once
# per service access
HSDIR_V2_ONION_ADDRESS_FETCH_PER_SERVICE = 1.0

# The entire hash ring of v3 HSDirs will see each descriptor id stored 3 times
# per service upload
HSDIR_V3_DESCRIPTOR_ID_STORE_PER_SERVICE = HSDIR_SPREAD

# The entire hash ring of v3 HSDirs will see each descriptor id fetched
# 0.5 times per service access
HSDIR_V3_DESCRIPTOR_ID_FETCH_PER_SERVICE = 1.0 / HSDIR_REPLICA

# v2 hidden services publish two descriptors whenever they publish in the last
# hour of each period. Periods are offset throughout each day, using the first
# byte of the service's address
HSDIR_V2_STORE_OVERLAP_FRACTION = 1.0 + 1.0/24.0

def hsdir_v2_descriptor_store_overlap():
    '''
    Returns the v2 HSDir overlap proportion.
    '''
    # v2 overlaps are evenly distributed
    return HSDIR_V2_STORE_OVERLAP_FRACTION

# v3 hidden services publish a new descriptor every 24 hours. Periods start
# when the service downloads a consensus with a new SRV, and finish 36 hours
# later.
HSDIR_V3_STORE_OVERLAP_FRACTION = 1.0 + 12.0/24.0

# Average client consensus download delay, based on 1-3 hour delay
# This is approximate: a more precise figure can be calculated from:
# https://gitweb.torproject.org/torspec.git/tree/dir-spec.txt#n3345
CLIENT_CONSENSUS_DELAY_TIMEDELTA = datetime.timedelta(hours=2)

# v3 overlaps occur in from 00:00 to 12:00 UTC
# https://gitweb.torproject.org/torspec.git/tree/proposals/224-rend-spec-ng.txt#n757
HSDIR_V3_STORE_OVERLAP_START_TIME = datetime.time(hour=00)
HSDIR_V3_STORE_OVERLAP_END_TIME   = datetime.time(hour=12)

def hsdir_v3_descriptor_store_overlap(valid_after):
    '''
    Returns the v3 HSDir overlap proportion for valid_after.
    '''
    assert valid_after >= datetime.datetime.min
    valid_after -= CLIENT_CONSENSUS_DELAY_TIMEDELTA
    if (valid_after.time() > HSDIR_V3_STORE_OVERLAP_START_TIME and
        valid_after.time() <= HSDIR_V3_STORE_OVERLAP_END_TIME):
        return HSDIR_V3_STORE_OVERLAP_FRACTION
    else:
        return 1.0

ALL_FINGERPRINTS = ['-']

def main():
    print "running weights privcount version {}".format(get_privcount_version())

    args = get_args()

    cons_desc = "consensuses '{}'".format(args.consensus)
    if args.consensus == '-':
        cons_desc = "latest consensus ('-')"

    fp_desc = "fingerprints '{}'".format(args.fingerprints)
    if len(args.fingerprints) == 0:
        fp_desc = "no fingerprints (empty list)"
    elif args.fingerprints == ALL_FINGERPRINTS:
        fp_desc = "all fingerprints ('-')"

    print "using {} and {}".format(cons_desc, fp_desc)

    if args.consensus == '-':
        args.consensus = fetch_consensus()

    expanded_path = os.path.abspath(os.path.expanduser(args.consensus))
    assert os.path.exists(expanded_path)

    consensus_paths = []
    if os.path.isdir(expanded_path):
        for filename in os.listdir(expanded_path):
            if filename.endswith("consensus"):
                consensus_paths.append(os.path.join(expanded_path, filename))
        assert len(consensus_paths) > 0, "consensus file names must end with 'consensus'"
    else:
        consensus_paths.append(expanded_path)

    # Use all fingerprints for testing purposes
    if args.fingerprints == ALL_FINGERPRINTS:
        fps = None
    else:
        fps = [fp_str.strip('$') for fp_str in args.fingerprints]
    work_items = [[path, fps] for path in consensus_paths]
    results = []
    if args.use_process_pool:
        p = Pool(cpu_count())
        try:
            results = do_map(p, work_items)
        except KeyboardInterrupt:
            print >> sys.stderr, "interrupted, terminating process pool"
            p.terminate()
            p.join()
            sys.exit(1)
    else:
        for params in work_items:
            result = process_consensus(params)
            results.append(result)

    guard_fracs, middle_fracs, exit_fracs, hsdir2_fracs, hsdir2_svc_fracs, hsdir3_fracs, hsdir3_svc_fracs, intro_fracs, rend2_fracs, rend3_fracs = do_reduce(results)
    assert len(guard_fracs) > 0

    print "{:.15f} : guard use mean".format(mean(guard_fracs))
    print "{:.15f} : middle use mean".format(mean(middle_fracs))
    print "{:.15f} : exit use mean".format(mean(exit_fracs))
    print "{:.15f} : hsdir2 spread-weighted service onion address mean (6 stores per onion address, 1/24 overlap)".format(mean(hsdir2_svc_fracs)*HSDIR_V2_ONION_ADDRESS_STORE_PER_SERVICE)
    print "{:.15f} : hsdir2 spread-weighted client onion address mean (1 fetch per onion address)".format(mean(hsdir2_fracs)*HSDIR_V2_ONION_ADDRESS_FETCH_PER_SERVICE)
    print "{:.15f} : hsdir3 relay fraction mean".format(mean(hsdir3_fracs))
    print "{:.15f} : hsdir3 approximate service descriptor id mean (3 stores per descriptor id, per onion address, 00:00 - 12:00 overlap)".format(mean(hsdir3_svc_fracs)*HSDIR_V3_DESCRIPTOR_ID_STORE_PER_SERVICE)
    print "{:.15f} : hsdir3 approximate client descriptor id mean (0.5 fetches per descriptor id, per onion address)".format(mean(hsdir3_fracs)*HSDIR_V3_DESCRIPTOR_ID_FETCH_PER_SERVICE)
    print "{:.15f} : intro2,3 service establish mean (default 3 intro points per onion address, max 10)".format(mean(intro_fracs)*INTRO_ESTABLISH_PER_SERVICE)
    print "{:.15f} : intro2,3 client introduce mean (1 introduce per onion address)".format(mean(intro_fracs))
    print "{:.15f} : rend2 use mean".format(mean(rend2_fracs))
    print "{:.15f} : rend3 use mean".format(mean(rend3_fracs))

def fetch_consensus():
    downloader = DescriptorDownloader()
    consensus = downloader.get_consensus(document_handler = DocumentHandler.DOCUMENT).run()[0]

    time_label = str(consensus.valid_after).replace(':', '-').replace(' ', '-')
    file_path = "{}-consensus".format(time_label)

    if not os.path.exists(file_path):
        with open(file_path, 'w') as descriptor_file:
          descriptor_file.write("@type network-status-consensus-3 1.0\n" + str(consensus))

    return file_path

def do_map(pool, work_items):
    async_result = pool.map_async(process_consensus, work_items)
    while not async_result.ready():
        async_result.wait(1)
    return async_result.get()

# this func is run by helper processes in process pool
def process_consensus(params):
    consensus_path, prints = params[0], params[1]
    guard_frac, middle_frac, exit_frac, hsdir2_frac, hsdir2_svc_frac, hsdir3_frac, hsdir3_svc_frac, intro_frac, rend2_frac, rend3_frac = get_fractional_weights(consensus_path, prints)
    return [guard_frac, middle_frac, exit_frac, hsdir2_frac, hsdir2_svc_frac, hsdir3_frac, hsdir3_svc_frac, intro_frac, rend2_frac, rend3_frac]

def do_reduce(async_results):
    guard_fracs, middle_fracs, exit_fracs, hsdir2_fracs, hsdir2_svc_fracs, hsdir3_fracs, hsdir3_svc_fracs, intro_fracs, rend2_fracs, rend3_fracs = [], [], [], [], [], [], [], [], [], []
    for result in async_results:
        if result is None: continue
        guard_frac, middle_frac, exit_frac, hsdir2_frac, hsdir2_svc_frac, hsdir3_frac, hsdir3_svc_frac, intro_frac, rend2_frac, rend3_frac = result[0], result[1], result[2], result[3], result[4], result[5], result[6], result[7], result[8], result[9]
        guard_fracs.append(guard_frac)
        middle_fracs.append(middle_frac)
        exit_fracs.append(exit_frac)
        hsdir2_fracs.append(hsdir2_frac)
        hsdir2_svc_fracs.append(hsdir2_svc_frac)
        hsdir3_fracs.append(hsdir3_frac)
        hsdir3_svc_fracs.append(hsdir3_svc_frac)
        intro_fracs.append(intro_frac)
        rend2_fracs.append(rend2_frac)
        rend3_fracs.append(rend3_frac)

    return guard_fracs, middle_fracs, exit_fracs, hsdir2_fracs, hsdir2_svc_fracs, hsdir3_fracs, hsdir3_svc_fracs, intro_fracs, rend2_fracs, rend3_fracs

def safe_d(dividend, divisor):
    dividend = float(dividend)
    divisor = float(divisor)
    if divisor == 0.0:
        return 0.0
    else:
        return dividend/divisor

def get_fractional_weights(consensus_path, my_fingerprints):
    # returns a tuple with guard, middle, exit, hsdir2, hsdir3, intro, rend2, and rend3 fractions
    # for my_fingerprints. If my_fingerprints is None, uses all fingerprints
    # in the consensus. (This is used for testing.)
    net_status = next(parse_file(consensus_path, document_handler='DOCUMENT', validate=False))
    bw_weight_scale = net_status.params['bwweightscale'] if 'bwweightscale' in net_status.params else 1.0

    my_guard, total_guard, my_middle, total_middle, my_exit, total_exit = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
    # this is the integer number of HSDirs (they don't use bandwidth weights)
    my_hsdir2, total_hsdir2, my_hsdir3, total_hsdir3 = 0.0, 0.0, 0.0, 0.0
    # these use bandwidth weights
    my_intro, total_intro, my_rend2, total_rend2, my_rend3, total_rend3 = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0

    if my_fingerprints is not None:
        for fingerprint in sorted(my_fingerprints):
            if fingerprint not in net_status.routers.keys():
                print "{} is not in consensus '{}'".format(fingerprint,
                                                           consensus_path)

    hsdir2_list = [r.fingerprint for r in net_status.routers.values() if is_hsdir_v2(r.flags)]
    hsdir2_ring = sorted(hsdir2_list)

    for (fingerprint, router_entry) in net_status.routers.items():
        #print fingerprint
        (guard_weighted_bw, middle_weighted_bw, exit_weighted_bw, hsdir2_spread, hsdir3_count, intro_weighted_bw, rend2_weighted_bw, rend3_weighted_bw) = get_weighted_bandwidths(router_entry, net_status.bandwidth_weights, bw_weight_scale, fingerprint, hsdir2_ring)
        total_guard += guard_weighted_bw
        total_middle += middle_weighted_bw
        total_exit += exit_weighted_bw
        total_hsdir2 += hsdir2_spread
        total_hsdir3 += hsdir3_count
        total_intro += intro_weighted_bw
        total_rend2 += rend2_weighted_bw
        total_rend3 += rend3_weighted_bw
        if my_fingerprints is None or fingerprint in my_fingerprints:
            my_guard += guard_weighted_bw
            my_middle += middle_weighted_bw
            my_exit += exit_weighted_bw
            my_hsdir2 += hsdir2_spread
            my_hsdir3 += hsdir3_count
            my_intro += intro_weighted_bw
            my_rend2 += rend2_weighted_bw
            my_rend3 += rend3_weighted_bw

    # scale HSDir service uploads to account for descriptor store overlaps
    my_hsdir2_svc = my_hsdir2*hsdir_v2_descriptor_store_overlap()
    my_hsdir3_svc = my_hsdir3*hsdir_v3_descriptor_store_overlap(
                                                       net_status.valid_after)

    return safe_d(my_guard, total_guard), safe_d(my_middle, total_middle), safe_d(my_exit, total_exit), safe_d(my_hsdir2, total_hsdir2), safe_d(my_hsdir2_svc, total_hsdir2), safe_d(my_hsdir3, total_hsdir3), safe_d(my_hsdir3_svc, total_hsdir3), safe_d(my_intro, total_intro), safe_d(my_rend2, total_rend2), safe_d(my_rend3, total_rend3)

def get_weighted_bandwidths(router_entry, bw_weights, bw_weight_scale, hsdir2_hash, hsdir2_ring):
    # example bw_weights: {u'Web': 10000, u'Wdb': 10000, u'Weg': 10000, u'Wee': 10000, u'Wed': 10000, u'Wgd': 0, u'Wgb': 10000, u'Wgg': 5920, u'Wem': 10000, u'Wbg': 4080, u'Wbd': 0, u'Wbe': 0, u'Wmm': 10000, u'Wmb': 10000, u'Wgm': 5920, u'Wbm': 10000, u'Wmg': 4080, u'Wme': 0, u'Wmd': 0}
    # returns a tuple with guard, middle, and exit weighted bandwidth, hsdir2 spread, hsdir3 count, and intro, rend2, and rend3 weighted bandwidth
    guard_weighted_bw, middle_weighted_bw, exit_weighted_bw = 0.0, 0.0, 0.0
    # this is the proportion of the hash ring allocated to the HSDir v2s spread
    hsdir2_spread = 0.0
    # this is the integer number of HSDir v3s (they don't use bandwidth weights)
    hsdir3_count = 0.0
    # these use bandwidth weights
    intro_weighted_bw, rend2_weighted_bw, rend3_weighted_bw = 0.0, 0.0, 0.0

    bw = float(router_entry.bandwidth)
    version = router_entry.version
    protocols = router_entry.protocols

    if Flag.GUARD in router_entry.flags and Flag.FAST in router_entry.flags and Flag.STABLE in router_entry.flags:
        guard_weight = safe_d(get_bw_weight(router_entry.flags, 'g', bw_weights), bw_weight_scale)
        guard_weighted_bw = bw * guard_weight

    middle_weight = safe_d(get_bw_weight(router_entry.flags, 'm', bw_weights), bw_weight_scale)
    middle_weighted_bw = bw * middle_weight

    if 'BadExit' not in router_entry.flags:
        if router_entry.exit_policy.is_exiting_allowed():
            exit_weight = safe_d(get_bw_weight(router_entry.flags, 'e', bw_weights), bw_weight_scale)
            exit_weighted_bw = bw * exit_weight

    hsdir2_spread = float(get_bw_weight(router_entry.flags, 'h2', None,
                                        hsdir2_hash=hsdir2_hash,
                                        hsdir2_ring=hsdir2_ring))
    hsdir3_count = float(get_bw_weight(router_entry.flags, 'h3', None,
                                       version=version, protocols=protocols))

    intro_weight = safe_d(get_bw_weight(router_entry.flags, 'i', bw_weights), bw_weight_scale)
    intro_weighted_bw = bw * intro_weight

    rend2_weight = safe_d(get_bw_weight(router_entry.flags, 'r2', bw_weights), bw_weight_scale)
    rend2_weighted_bw = bw * rend2_weight

    rend3_weight = safe_d(get_bw_weight(router_entry.flags, 'r3', bw_weights, protocols=protocols),
                          bw_weight_scale)
    rend3_weighted_bw = bw * rend3_weight

    return guard_weighted_bw, middle_weighted_bw, exit_weighted_bw, hsdir2_spread, hsdir3_count, intro_weighted_bw, rend2_weighted_bw, rend3_weighted_bw

PROTOVER_HSREND_V3 = 2
PROTOVER_HSDIR_V3 = 2

def is_hsdir_v2(flags):
    '''
    Is a relay with flags an HSDir v2?
    '''
    return Flag.HSDIR in flags

def get_ring_index(index, ring):
    '''
    Get index from ring, treating it as a circular list
    '''
    # We can't loop more than once, because we won't know how many times
    # we've looped. This means we must have at least HSDIR_SPREAD HSDirs
    # in our hash ring
    assert len(ring) >= HSDIR_SPREAD
    modulus = len(ring)
    valid_index = index % modulus
    return ring[valid_index]

def get_ring_size(arbitrary_hash):
    '''
    Calculate the spread amount of the entire hash ring, based on
    arbitrary_hash, which is a hex-encoded hash with leading zeroes.
    Returns a long.
    '''
    return 2L**(4*len(arbitrary_hash))

def get_ring_spread(later_hash, earlier_hash):
    '''
    Calculate the spread amount between later and earlier, which are
    hex-encoded hashes in a ring. Returns a long.
    '''
    assert len(later_hash) == len(earlier_hash)
    assert HSDIR_SPREAD > 0
    hash_modulus = get_ring_size(later_hash)
    spread_amount = (long(later_hash, 16) - long(earlier_hash, 16)) % hash_modulus
    # If we've looped the whole way around
    if spread_amount == 0:
        return hash_modulus
    else:
        return spread_amount

def get_hsdir_spread(hash, ring):
    '''
    Return the proportion of ring occupied by the HSDir spread allocated to
    hash, as a floating point fraction, accurate to at least the first
    sys.float_info.mant_dig (53) bits of the hashes involved.
    '''
    assert hash is not None
    assert ring is not None
    assert hash in ring

    # find the hashes in the spread
    hash_index = ring.index(hash)
    spread_index = hash_index - HSDIR_SPREAD
    spread_hash = get_ring_index(spread_index, ring)

    # find the integer-accurate spread amount and ring size
    spread_amount = get_ring_spread(hash, spread_hash)
    ring_size = get_ring_size(hash)

    # make them into floating-point numbers
    # when ring_size is less than the floating-point exponent, we don't lose
    # any precision here, because it can be represented exactly. If it can't
    # be represented exactly, we might end up dividing by zero.
    assert ring_size <= 2**abs(sys.float_info.min_exp)
    ring_size = float(ring_size)
    # this rounds the mantissa to the nearest sys.float_info.mant_dig bits,
    # preserving at least 53 bits of the hashes (there may be more if the
    # spread amount has leading zeroes)
    spread_amount = float(spread_amount)
    return spread_amount / ring_size

# from torps
def get_bw_weight(flags, position, bw_weights, version=None, protocols=None,
                  hsdir2_hash=None, hsdir2_ring=None):
    """Returns weight to apply to relay's bandwidth for given position, for
        HSDir v2, returns the spread proportion for hsdir2_hash in hsdir2_ring
        hash ring, or for HSDir v3, returns 1.0 when a relay has the flag, and
        0.0 when it does not
        (version and protocols are used to check if a relay supports HSDir and Rend v3)
        flags: list of Flag values for relay from a consensus
        position: position for which to find selection weight,
             one of 'g' for guard, 'm' for middle, 'e' for exit,
             'h{2,3}' for HSDir v2 and v3, 'i' for intro, 'r{2,3}' for Rend v2 and v3
        bw_weights: bandwidth_weights from NetworkStatusDocumentV3 consensus,
             or None for HSDirs
    """

    if (position == 'g'):
        if (Flag.GUARD in flags) and (Flag.EXIT in flags):
            return bw_weights['Wgd']
        elif (Flag.GUARD in flags):
            return bw_weights['Wgg']
        elif (Flag.EXIT not in flags):
            return bw_weights['Wgm']
        else:
            # exit-flagged nodes without guard flag never serve in guard position
            #raise ValueError('Wge weight does not exist.')
            return 0
    elif (position == 'm' or position == 'i' or position.startswith('r')):
        # intro points must have the Stable flag
        if (position == 'i' and not Flag.STABLE in flags):
            return 0.0
        # rend3 points must support the rend v3 protocol
        elif (position == 'r3' and
              (protocols is None or
               PROTOVER_HSREND_V3 not in protocols.get('HSRend',[]))):
            return 0.0
        elif (Flag.GUARD in flags) and (Flag.EXIT in flags):
            return bw_weights['Wmd']
        elif (Flag.GUARD in flags):
            return bw_weights['Wmg']
        elif (Flag.EXIT in flags):
            return bw_weights['Wme']
        else:
            return bw_weights['Wmm']
    elif (position == 'e'):
        if (Flag.GUARD in flags) and (Flag.EXIT in flags):
            return bw_weights['Wed']
        elif (Flag.GUARD in flags):
            return bw_weights['Weg']
        elif (Flag.EXIT in flags):
            return bw_weights['Wee']
        else:
            return bw_weights['Wem']
    elif (position == 'h2'):
        if is_hsdir_v2(flags):
            return get_hsdir_spread(hsdir2_hash, hsdir2_ring)
        else:
            return 0.0
    elif (position == 'h3'):
        # There is no HSDir3 flag. Instead, HSDir v3 must have the HSDir flag,
        # the HSDir protocol version 2, and a tor version >= 0.3.0.8.
        # https://trac.torproject.org/projects/tor/ticket/23340
        if (Flag.HSDIR in flags):
            if (version < Version('0.3.0.8')):
                return 0.0
            elif (protocols is not None and
                  PROTOVER_HSDIR_V3 in protocols.get('HSDir',[])):
                return 1.0
            else:
                return 0.0
        else:
            return 0.0
    else:
        raise ValueError('get_weight does not support position {0}.'.format(
            position))

class CustomHelpFormatter(ArgumentDefaultsHelpFormatter):
    # adds the 'RawDescriptionHelpFormatter' to the ArgsDefault one
    def _fill_text(self, text, width, indent):
        return ''.join([indent + line for line in text.splitlines(True)])

def get_args():
    parser = ArgumentParser(
            description='Compute weighted fractional position bandwidths for set of relays',
            formatter_class=CustomHelpFormatter)

    parser.add_argument('consensus', help="Path to a consensus file or a directory containing multiple consensus files, or '-' to download and use the latest Tor consensus", metavar="PATH")
    parser.add_argument('fingerprints', help="Fingerprints of 0 or more relays to include in reported fractions, or '-' to include all relays", metavar="FP", nargs='*')
    parser.add_argument('-m', action='store_true', dest="use_process_pool", help="Run with a {} process pool".format(cpu_count()))

    args = parser.parse_args()
    return args

if __name__ == "__main__":
    sys.exit(main())
